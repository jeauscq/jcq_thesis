{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92dbe93",
   "metadata": {},
   "source": [
    "This code is used to compare two files and print the statistics of the differences. It takes into consideration possibly failed trajectories that are documented in a third file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df66bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7379ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_trajectories(path: Path) -> List[np.ndarray]:\n",
    "    \"\"\"Load a CSV file where each row is a trajectory of semicolon-separated states.\"\"\"\n",
    "    trajectories = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            steps = line.strip().split(',')\n",
    "            trajectory = [list(map(float, s.split(';'))) for s in steps]\n",
    "            trajectories.append(np.array(trajectory))\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e221bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_failed_indices(path: Optional[Path]) -> set:\n",
    "    \"\"\"Load indices of failed trajectories from a text file.\"\"\"\n",
    "    if path is None:\n",
    "        return set()\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            return set(int(line.strip()) for line in f if line.strip().isdigit())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File {path} not found. Returning empty set.\")\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5e79e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_metrics(ref_traj: List[np.ndarray], eval_traj: List[np.ndarray], failed: set) -> Tuple[dict, List[float], pd.DataFrame]:\n",
    "    \"\"\"Compute RMSE statistics per trajectory, excluding failed ones.\"\"\"\n",
    "    skipped = 0\n",
    "    per_traj_rmses = []\n",
    "    traj_stats = []\n",
    "    for idx in range(len(ref_traj)):\n",
    "        if idx in failed:\n",
    "            skipped += 1\n",
    "            print(f\"Skipping failed trajectory {idx+1}.\")\n",
    "        if idx >= len(eval_traj):\n",
    "            print(f\"Finished: All the trajectories were evaluated already at trajectory {idx+1}.\")\n",
    "            break\n",
    "\n",
    "        ref = ref_traj[idx+skipped]\n",
    "        eval_ = eval_traj[idx]\n",
    "\n",
    "        if ref.shape[0] != eval_.shape[0]:\n",
    "            if ref.shape[0] > eval_.shape[0]:\n",
    "                ref = ref[1:, :]\n",
    "            else:\n",
    "                print(f\"Warning: Reference trajectory {idx} is shorter than evaluated trajectory. Skipping.\")\n",
    "                continue\n",
    "\n",
    "        if ref.shape != eval_.shape:\n",
    "            print(f\"Warning: Shape mismatch at trajectory {idx}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        error = ref - eval_\n",
    "        rmse = np.sqrt(np.mean(np.square(error)))\n",
    "        per_traj_rmses.append(rmse)\n",
    "        traj_stats.append({\n",
    "            'trajectory': idx,\n",
    "            'rmse': rmse,\n",
    "            'std': np.std(error),\n",
    "            'max': np.max(np.abs(error))\n",
    "        })\n",
    "\n",
    "    # Global metrics\n",
    "    global_metrics = {\n",
    "        'global_rmse_mean': np.mean(per_traj_rmses),\n",
    "        'global_rmse_std': np.std(per_traj_rmses),\n",
    "        'global_rmse_max': np.max(per_traj_rmses),\n",
    "    }\n",
    "\n",
    "    return global_metrics, per_traj_rmses, pd.DataFrame(traj_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e651cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_trajectories(reference_path: Path, constrained_path: Path, failed_path: Optional[Path] = None, verbose: bool = True):\n",
    "    \"\"\"Main function to compare reference and generated trajectories.\"\"\"\n",
    "    reference = load_trajectories(reference_path)\n",
    "    constrained = load_trajectories(constrained_path)\n",
    "    if failed_path != \"\":\n",
    "        failed = load_failed_indices(failed_path)\n",
    "    else:\n",
    "        failed = set()\n",
    "\n",
    "    global_metrics, per_traj_rmses, traj_df = compute_rmse_metrics(reference, constrained, failed)\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(per_traj_rmses, bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.title(\"Per-Trajectory RMSE Histogram\")\n",
    "    plt.xlabel(\"RMSE\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    if verbose:\n",
    "        histogram_path = Path(\"rmse_histogram.pdf\")\n",
    "        plt.savefig(histogram_path)\n",
    "    plt.close()\n",
    "\n",
    "    if verbose:\n",
    "        # Save trajectory-wise metrics\n",
    "        traj_df.to_csv(\"trajectory_rmse_stats.csv\", index=False)\n",
    "\n",
    "    return global_metrics, traj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af69edc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'global_rmse_mean': 0.0583133431338275, 'global_rmse_std': 0.05844708784972515, 'global_rmse_max': 1.4603586645523172}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATASET_DIR = \"/home/jeauscq/Desktop/jcq_thesis/datasets/\"\n",
    "ref_path = DATASET_DIR + \"Policy/training_trajectories_policy_n.csv\"\n",
    "gen_path = DATASET_DIR + \"Policy/MPC/torque_Const/mpc_generated_tor_constraint_dataset_n.csv\"\n",
    "fail_path = DATASET_DIR + \"Policy/MPC/pos_vel_Const/failed_constrained_trajectories.txt\"\n",
    "\n",
    "metrics, traj_df = compare_trajectories(ref_path, gen_path, \"\", verbose=False)\n",
    "print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
